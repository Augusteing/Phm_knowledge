# -*- coding: utf-8 -*-
"""
RAG æ¨¡å‹ç»“æœå¯¹æ¯”åˆ†æè„šæœ¬
ç»Ÿè®¡ DeepSeekã€Geminiã€Kimi ä¸‰ä¸ªæ¨¡å‹çš„æŠ½å–ç»“æœ
ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼å’Œå¯è§†åŒ–å›¾è¡¨
"""
import os
import json
import argparse
from pathlib import Path
from collections import defaultdict
import pandas as pd
from datetime import datetime

# ------------------------------
# è·¯å¾„é…ç½®
# ------------------------------
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent  # .../exp01_baseline

def resolve_outputs_root(cli_outputs_dir: str | None) -> Path:
    if cli_outputs_dir:
        return Path(cli_outputs_dir).resolve()
    return PROJECT_ROOT / "outputs"

def ensure_output_dir(outputs_root: Path) -> Path:
    out = outputs_root / "analysis" / "statistics"
    os.makedirs(out, exist_ok=True)
    return out

# ------------------------------
# æ•°æ®ç»Ÿè®¡å‡½æ•°
# ------------------------------
def count_json_files(directory: Path) -> dict:
    """ç»Ÿè®¡ç›®å½•ä¸‹çš„JSONæ–‡ä»¶æ•°é‡å’ŒåŸºæœ¬ä¿¡æ¯"""
    if not directory.exists():
        return {
            "total": 0,
            "success": 0,
            "failed": 0,
            "files": []
        }
    # é€’å½’è¯»å–æ‰€æœ‰å­ç›®å½•
    json_files = list(directory.rglob("*.json"))
    # å¤±è´¥æ ‡è®°å…¼å®¹ä¸¤ç§ï¼š.error.txt æˆ– .failed.txt
    error_files = list(directory.rglob("*.error.txt")) + list(directory.rglob("*.failed.txt"))
    
    return {
        "total": len(json_files) + len(error_files),
        "success": len(json_files),
        "failed": len(error_files),
        "files": [f.name for f in json_files]
    }

def analyze_json_content(file_path: Path) -> dict:
    """åˆ†æå•ä¸ªJSONæ–‡ä»¶çš„å†…å®¹"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        entities = data.get("entities", [])
        relations = data.get("relations", [])
        
        # ç»Ÿè®¡å®ä½“ç±»å‹åˆ†å¸ƒ
        entity_types = defaultdict(int)
        for entity in entities:
            entity_type = entity.get("type", "unknown")
            entity_types[entity_type] += 1
        
        # ç»Ÿè®¡å…³ç³»ç±»å‹åˆ†å¸ƒ
        relation_types = defaultdict(int)
        for relation in relations:
            relation_type = relation.get("relation", "unknown")
            relation_types[relation_type] += 1
        
        return {
            "entity_count": len(entities),
            "relation_count": len(relations),
            "entity_type_count": len(entity_types),  # å®ä½“ç±»å‹æ•°é‡
            "relation_type_count": len(relation_types),  # å…³ç³»ç±»å‹æ•°é‡
            "entity_types": dict(entity_types),
            "relation_types": dict(relation_types)
        }
    except Exception as e:
        print(f"âš ï¸ åˆ†ææ–‡ä»¶å¤±è´¥ {file_path.name}: {e}")
        return {
            "entity_count": 0,
            "relation_count": 0,
            "entity_type_count": 0,
            "relation_type_count": 0,
            "entity_types": {},
            "relation_types": {}
        }

def analyze_model_results(model_name: str, json_dir: Path, log_dir: Path) -> dict:
    """åˆ†æå•ä¸ªæ¨¡å‹çš„æ‰€æœ‰ç»“æœ"""
    print(f"\nğŸ“Š åˆ†æ {model_name} æ¨¡å‹ç»“æœ...")
    
    # åŸºæœ¬ç»Ÿè®¡
    file_stats = count_json_files(json_dir)
    print(f"   - æˆåŠŸ: {file_stats['success']} ç¯‡")
    print(f"   - å¤±è´¥: {file_stats['failed']} ç¯‡")
    
    # è¯¦ç»†åˆ†ææ¯ä¸ªJSONæ–‡ä»¶
    detailed_results = []
    total_entities = 0
    total_relations = 0
    total_entity_types = 0
    total_relation_types = 0
    
    # ç”¨äºç»Ÿè®¡æ‰€æœ‰è®ºæ–‡ä¸­çš„ç±»å‹ï¼ˆå»é‡ï¼‰
    all_entity_types = set()
    all_relation_types = set()
    
    for json_file in json_dir.rglob("*.json"):
        paper_name = json_file.stem
        analysis = analyze_json_content(json_file)
        
        detailed_results.append({
            "paper": paper_name,
            "entities": analysis["entity_count"],
            "relations": analysis["relation_count"],
            "entity_types": analysis["entity_type_count"],
            "relation_types": analysis["relation_type_count"]
        })
        
        total_entities += analysis["entity_count"]
        total_relations += analysis["relation_count"]
        total_entity_types += analysis["entity_type_count"]
        total_relation_types += analysis["relation_type_count"]
        
        # æ”¶é›†æ‰€æœ‰ç±»å‹
        all_entity_types.update(analysis["entity_types"].keys())
        all_relation_types.update(analysis["relation_types"].keys())
    
    # è®¡ç®—å¹³å‡å€¼
    success_count = file_stats['success']
    avg_entities = total_entities / success_count if success_count > 0 else 0
    avg_relations = total_relations / success_count if success_count > 0 else 0
    avg_entity_types = total_entity_types / success_count if success_count > 0 else 0
    avg_relation_types = total_relation_types / success_count if success_count > 0 else 0
    
    return {
        "model": model_name,
        "total_papers": file_stats['total'],
        "success": file_stats['success'],
        "failed": file_stats['failed'],
        "total_entities": total_entities,
        "total_relations": total_relations,
        "avg_entities": round(avg_entities, 2),
        "avg_relations": round(avg_relations, 2),
        "total_entity_types": total_entity_types,
        "total_relation_types": total_relation_types,
        "avg_entity_types": round(avg_entity_types, 2),
        "avg_relation_types": round(avg_relation_types, 2),
        "unique_entity_types": len(all_entity_types),
        "unique_relation_types": len(all_relation_types),
        "detailed_results": detailed_results
    }

# ------------------------------
# ä¸»ç¨‹åº
# ------------------------------
def main():
    parser = argparse.ArgumentParser(description="RAG ç»“æœå¯¹æ¯”åˆ†æ")
    parser.add_argument(
        "--outputs-dir",
        help="è¦†ç›– outputs æ ¹ç›®å½•ï¼Œé»˜è®¤ä½¿ç”¨å½“å‰å®éªŒç›®å½•ä¸‹çš„ outputs",
        default=None,
    )
    args = parser.parse_args()

    outputs_root = resolve_outputs_root(args.outputs_dir)
    # ä¸‰ä¸ªæ¨¡å‹çš„è¾“å‡ºç›®å½•ï¼ˆåŸºäº outputsï¼‰
    deepseek_dir = outputs_root / "extractions" / "deepseek"
    gemini_dir = outputs_root / "extractions" / "gemini"
    kimi_dir = outputs_root / "extractions" / "kimi"

    # æ—¥å¿—ç›®å½•
    deepseek_log = outputs_root / "logs" / "deepseek"
    gemini_log = outputs_root / "logs" / "gemini"
    kimi_log = outputs_root / "logs" / "kimi"

    output_dir = ensure_output_dir(outputs_root)
    print("=" * 80)
    print("ğŸ”¬ RAG æ¨¡å‹æŠ½å–ç»“æœç»Ÿè®¡åˆ†æ")
    print("=" * 80)
    
    # åˆ†æä¸‰ä¸ªæ¨¡å‹
    models_config = [
        ("DeepSeek", deepseek_dir, deepseek_log),
        ("Gemini", gemini_dir, gemini_log),
        ("Kimi", kimi_dir, kimi_log),
    ]
    
    all_results = []
    
    for model_name, json_dir, log_dir in models_config:
        result = analyze_model_results(model_name, json_dir, log_dir)
        all_results.append(result)
        
        # ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºå­ç›®å½•å¹¶ä¿å­˜è®ºæ–‡ç»Ÿè®¡
        model_output_dir = output_dir / model_name.lower()
        os.makedirs(model_output_dir, exist_ok=True)
        
        paper_stats_df = pd.DataFrame(result["detailed_results"])
        paper_stats_file = model_output_dir / "paper_statistics.csv"
        paper_stats_df.to_csv(paper_stats_file, index=False, encoding='utf-8-sig')
        print(f"   âœ… å·²ä¿å­˜: {paper_stats_file}")
    
    # ------------------------------
    # ç”Ÿæˆæ±‡æ€»è¡¨æ ¼
    # ------------------------------
    print("\n" + "=" * 80)
    print("ğŸ“‹ æ¨¡å‹å¯¹æ¯”æ±‡æ€»")
    print("=" * 80)
    
    summary_df = pd.DataFrame([
        {
            "æ¨¡å‹": r["model"],
            "æˆåŠŸè®ºæ–‡æ•°": r["success"],
            "å¤±è´¥è®ºæ–‡æ•°": r["failed"],
            "æ€»å®ä½“æ•°": r["total_entities"],
            "æ€»å…³ç³»æ•°": r["total_relations"],
            "å¹³å‡å®ä½“æ•°": r["avg_entities"],
            "å¹³å‡å…³ç³»æ•°": r["avg_relations"],
            "å¹³å‡å®ä½“ç±»å‹æ•°": r["avg_entity_types"],
            "å¹³å‡å…³ç³»ç±»å‹æ•°": r["avg_relation_types"],
            "æ€»å®ä½“ç±»å‹æ•°": r["unique_entity_types"],
            "æ€»å…³ç³»ç±»å‹æ•°": r["unique_relation_types"]
        }
        for r in all_results
    ])
    
    print(summary_df.to_string(index=False))
    
    # ------------------------------
    # ç”Ÿæˆæ±‡æ€» Markdown æŠ¥å‘Šï¼ˆæ”¾åœ¨å¤–å±‚ï¼‰
    # ------------------------------
    report_file = output_dir / "extraction_summary.md"
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# RAG æ¨¡å‹æŠ½å–ç»“æœç»Ÿè®¡æŠ¥å‘Š\n\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("## 1. æ¨¡å‹å¯¹æ¯”æ±‡æ€»\n\n")
        # æ‰‹åŠ¨ç”Ÿæˆ Markdown è¡¨æ ¼
        headers = summary_df.columns.tolist()
        f.write("| " + " | ".join(headers) + " |\n")
        f.write("|" + "|".join(["---"] * len(headers)) + "|\n")
        for _, row in summary_df.iterrows():
            f.write("| " + " | ".join(str(v) for v in row.values) + " |\n")
        f.write("\n")
        
        f.write("## 2. å…³é”®å‘ç°\n\n")
        
        best_entities = max(all_results, key=lambda x: x["avg_entities"])
        best_relations = max(all_results, key=lambda x: x["avg_relations"])
        best_entity_types = max(all_results, key=lambda x: x["avg_entity_types"])
        best_relation_types = max(all_results, key=lambda x: x["avg_relation_types"])
        most_diverse_entity = max(all_results, key=lambda x: x["unique_entity_types"])
        most_diverse_relation = max(all_results, key=lambda x: x["unique_relation_types"])
        
        f.write(f"- **æœ€å¤šå¹³å‡å®ä½“æ•°**: {best_entities['model']} ({best_entities['avg_entities']} ä¸ª)\n")
        f.write(f"- **æœ€å¤šå¹³å‡å…³ç³»æ•°**: {best_relations['model']} ({best_relations['avg_relations']} ä¸ª)\n")
        f.write(f"- **æœ€å¤šå¹³å‡å®ä½“ç±»å‹æ•°**: {best_entity_types['model']} ({best_entity_types['avg_entity_types']} ç§)\n")
        f.write(f"- **æœ€å¤šå¹³å‡å…³ç³»ç±»å‹æ•°**: {best_relation_types['model']} ({best_relation_types['avg_relation_types']} ç§)\n")
        f.write(f"- **æœ€ä¸°å¯Œå®ä½“ç±»å‹**: {most_diverse_entity['model']} (å…± {most_diverse_entity['unique_entity_types']} ç§ä¸åŒç±»å‹)\n")
        f.write(f"- **æœ€ä¸°å¯Œå…³ç³»ç±»å‹**: {most_diverse_relation['model']} (å…± {most_diverse_relation['unique_relation_types']} ç§ä¸åŒç±»å‹)\n\n")
        
        f.write("## 3. å„æ¨¡å‹è¯¦ç»†æ•°æ®\n\n")
        
        for r in all_results:
            model_name = r['model']
            f.write(f"### {model_name}\n\n")
            f.write(f"- æˆåŠŸè®ºæ–‡æ•°: {r['success']}\n")
            f.write(f"- æ€»å®ä½“æ•°: {r['total_entities']}\n")
            f.write(f"- æ€»å…³ç³»æ•°: {r['total_relations']}\n")
            f.write(f"- å¹³å‡å®ä½“æ•°: {r['avg_entities']}\n")
            f.write(f"- å¹³å‡å…³ç³»æ•°: {r['avg_relations']}\n")
            f.write(f"- å¹³å‡å®ä½“ç±»å‹æ•°: {r['avg_entity_types']}\n")
            f.write(f"- å¹³å‡å…³ç³»ç±»å‹æ•°: {r['avg_relation_types']}\n")
            f.write(f"- æ€»å®ä½“ç±»å‹æ•°ï¼ˆå»é‡ï¼‰: {r['unique_entity_types']}\n")
            f.write(f"- æ€»å…³ç³»ç±»å‹æ•°ï¼ˆå»é‡ï¼‰: {r['unique_relation_types']}\n")
            f.write(f"- è¯¦ç»†æ•°æ®: `{model_name.lower()}/paper_statistics.csv`\n\n")
        
        f.write("## 4. æ–‡ä»¶è¯´æ˜\n\n")
        f.write("```\n")
        f.write("statistics/\n")
        f.write("â”œâ”€â”€ extraction_summary.md         # æœ¬æ–‡ä»¶ï¼ˆæ±‡æ€»æŠ¥å‘Šï¼‰\n")
        f.write("â”œâ”€â”€ deepseek/\n")
        f.write("â”‚   â””â”€â”€ paper_statistics.csv     # DeepSeek æ¯ç¯‡è®ºæ–‡çš„ç»Ÿè®¡æ•°æ®\n")
        f.write("â”œâ”€â”€ gemini/\n")
        f.write("â”‚   â””â”€â”€ paper_statistics.csv     # Gemini æ¯ç¯‡è®ºæ–‡çš„ç»Ÿè®¡æ•°æ®\n")
        f.write("â””â”€â”€ kimi/\n")
        f.write("    â””â”€â”€ paper_statistics.csv     # Kimi æ¯ç¯‡è®ºæ–‡çš„ç»Ÿè®¡æ•°æ®\n")
        f.write("```\n\n")
        
        f.write("## 5. CSV æ–‡ä»¶åˆ—è¯´æ˜\n\n")
        f.write("- **paper**: è®ºæ–‡åç§°\n")
        f.write("- **entities**: å®ä½“æ•°é‡\n")
        f.write("- **relations**: å…³ç³»æ•°é‡\n")
        f.write("- **entity_types**: å®ä½“ç±»å‹æ•°é‡ï¼ˆå»é‡ï¼‰\n")
        f.write("- **relation_types**: å…³ç³»ç±»å‹æ•°é‡ï¼ˆå»é‡ï¼‰\n")
    
    print(f"\nâœ… æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    # æ‰“å°ç»Ÿè®¡äº®ç‚¹
    print("\n" + "=" * 80)
    print("ğŸ¯ ç»Ÿè®¡äº®ç‚¹")
    print("=" * 80)
    
    entity_ranking = sorted(all_results, key=lambda x: x["avg_entities"], reverse=True)
    relation_ranking = sorted(all_results, key=lambda x: x["avg_relations"], reverse=True)
    entity_type_ranking = sorted(all_results, key=lambda x: x["avg_entity_types"], reverse=True)
    relation_type_ranking = sorted(all_results, key=lambda x: x["avg_relation_types"], reverse=True)
    
    print("\nå¹³å‡å®ä½“æ•°æ’å:")
    for rank, r in enumerate(entity_ranking, 1):
        print(f"  {rank}. {r['model']}: {r['avg_entities']} ä¸ª")
    
    print("\nå¹³å‡å…³ç³»æ•°æ’å:")
    for rank, r in enumerate(relation_ranking, 1):
        print(f"  {rank}. {r['model']}: {r['avg_relations']} ä¸ª")
    
    print("\nå¹³å‡å®ä½“ç±»å‹æ•°æ’å:")
    for rank, r in enumerate(entity_type_ranking, 1):
        print(f"  {rank}. {r['model']}: {r['avg_entity_types']} ç§ (æ€»å…± {r['unique_entity_types']} ç§ä¸åŒç±»å‹)")
    
    print("\nå¹³å‡å…³ç³»ç±»å‹æ•°æ’å:")
    for rank, r in enumerate(relation_type_ranking, 1):
        print(f"  {rank}. {r['model']}: {r['avg_relation_types']} ç§ (æ€»å…± {r['unique_relation_types']} ç§ä¸åŒç±»å‹)")
    
    print("\n" + "=" * 80)
    print("âœ… åˆ†æå®Œæˆ!")
    print("=" * 80)
    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"   - æ±‡æ€»æŠ¥å‘Š: extraction_summary.md")
    print(f"   - DeepSeekç»Ÿè®¡: deepseek/paper_statistics.csv")
    print(f"   - Geminiç»Ÿè®¡: gemini/paper_statistics.csv")
    print(f"   - Kimiç»Ÿè®¡: kimi/paper_statistics.csv")

if __name__ == "__main__":
    main()
