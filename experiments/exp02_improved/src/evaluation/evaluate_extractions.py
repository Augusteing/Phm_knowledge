"""
å®ä½“å…³ç³»æŠ½å–ç»“æœè¯„ä¼°è„šæœ¬
ä½¿ç”¨ Gemini æ¨¡å‹å¯¹æŠ½å–ç»“æœè¿›è¡Œåˆç†æ€§è¯„ä¼°

æ›´æ–°ç‚¹ï¼š
- é‡‡ç”¨ EXP ç›®å½•å†…çš„ outputs ç»“æ„ï¼šoutputs/extractions/<model> â†’ outputs/evaluations/<model>
- é€’å½’æ‰«ææŠ½å–ç»“æœæ–‡ä»¶ï¼ˆæ”¯æŒå­ç›®å½•ï¼Œå¦‚ priority/generalï¼‰
- Prompt è·¯å¾„æ”¹ä¸ºï¼šEXP_DIR/config/prompt/prompt_eva.txtï¼ˆè‹¥ç¼ºå¤±åˆ™å›é€€æ—§è·¯å¾„ï¼‰
- æ”¯æŒ CLI å‚æ•°ï¼š--outputs-dir è¦†ç›– outputs æ ¹ç›®å½•ï¼›--models æŒ‡å®šè¯„ä¼°æ¨¡å‹åˆ—è¡¨
"""
import os
import json
import time
import argparse
from pathlib import Path
from datetime import datetime, timezone
from typing import Dict, List, Tuple, Optional
from tqdm import tqdm

# OpenAI SDK (Gemini å…¼å®¹æ¥å£)
from openai import OpenAI

# ------------------------------
# è·¯å¾„é…ç½®
# ------------------------------
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent  # exp01_baseline æ ¹ç›®å½•

# è¯„ä¼° Promptï¼Œä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æä¾›çš„æ–°ä½ç½®ï¼›è‹¥ä¸å­˜åœ¨åˆ™å°è¯•æ—§ä½ç½®
EVAL_PROMPT_PRIMARY = PROJECT_ROOT / "config" / "prompt" / "prompt_eva.txt"
EVAL_PROMPT_FALLBACK = PROJECT_ROOT / "configs" / "prompts" / "prompt_eva.txt"

# åŸå§‹è®ºæ–‡ç›®å½•ï¼ˆç”¨äºæä¾›ä¸Šä¸‹æ–‡ï¼Œå¦‚æœªæ¥éœ€è¦å¯ä½¿ç”¨ï¼‰
PAPERS_DIR = PROJECT_ROOT / "data" / "raw" / "papers"

# Gemini è¯„ä¼°é…ç½®
EVAL_MODEL = "gemini-2.5-pro"
PROVIDER_NAME = "gemini_evaluator"

# ------------------------------
# å·¥å…·å‡½æ•°
# ------------------------------
def now_iso() -> str:
    return datetime.now(timezone.utc).isoformat()

def strip_code_fences(s: str) -> str:
    """å»é™¤ ```json ... ``` æ ·å¼çš„ä»£ç å›´æ """
    if not isinstance(s, str):
        return s
    s = s.strip()
    if s.startswith("```") and s.endswith("```"):
        s = s[3:-3].strip()
        if "\n" in s:
            first_line, rest = s.split("\n", 1)
            if first_line.strip().lower() in {"json", "js", "javascript"}:
                s = rest
    # å°è¯•æå– { } ä¹‹é—´çš„å†…å®¹
    first_brace = s.find('{')
    last_brace = s.rfind('}')
    if first_brace != -1 and last_brace != -1 and first_brace < last_brace:
        s = s[first_brace:last_brace + 1]
    return s

def parse_json_response(content: str) -> dict:
    """è§£æ JSON å“åº”,è‡ªåŠ¨æ¸…ç†ä»£ç å›´æ """
    if not content:
        raise ValueError("API è¿”å›äº†ç©ºå†…å®¹")
    
    try:
        return json.loads(content)
    except json.JSONDecodeError:
        cleaned = strip_code_fences(content)
        try:
            return json.loads(cleaned)
        except json.JSONDecodeError as e:
            print(f"\nâŒ JSON è§£æå¤±è´¥: {e}")
            print(f"   åŸå§‹å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            print(f"   æ¸…ç†åå†…å®¹é•¿åº¦: {len(cleaned)} å­—ç¬¦")
            print(f"   æ¸…ç†åå†…å®¹é¢„è§ˆ: {cleaned[:500]}")
            raise ValueError(f"æ— æ³•è§£æ JSON: {e}")

def resolve_prompt_file() -> Path:
    if EVAL_PROMPT_PRIMARY.exists():
        return EVAL_PROMPT_PRIMARY
    if EVAL_PROMPT_FALLBACK.exists():
        return EVAL_PROMPT_FALLBACK
    raise FileNotFoundError(
        f"æœªæ‰¾åˆ°è¯„ä¼° Prompt æ–‡ä»¶ã€‚ä¼˜å…ˆè·¯å¾„: {EVAL_PROMPT_PRIMARY}ï¼›å›é€€è·¯å¾„: {EVAL_PROMPT_FALLBACK}"
    )

def init_client() -> OpenAI:
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise ValueError("è¯·è®¾ç½® GEMINI_API_KEY ç¯å¢ƒå˜é‡")
    return OpenAI(api_key=api_key, base_url="https://hiapi.online/v1")

# ------------------------------
# è¯„ä¼°å‡½æ•°
# ------------------------------
def evaluate_extraction(client: OpenAI, eval_prompt_template: str, extraction_data: dict, paper_name: str, model_name: str) -> dict:
    """
    ä½¿ç”¨ Gemini è¯„ä¼°å•ä¸ªæŠ½å–ç»“æœ
    
    Args:
        extraction_data: æŠ½å–çš„å®ä½“å’Œå…³ç³» JSON
        paper_name: è®ºæ–‡åç§°
        model_name: æŠ½å–æ¨¡å‹åç§°
    
    Returns:
        è¯„ä¼°åçš„ JSON (æ·»åŠ äº† evaluation å­—æ®µ)
    """
    # æ„å»ºè¯„ä¼° Prompt
    extraction_json = json.dumps(extraction_data, ensure_ascii=False, indent=2)
    
    eval_prompt = eval_prompt_template + f"""

## å¾…è¯„ä¼°çš„æŠ½å–ç»“æœ

è®ºæ–‡: {paper_name}
æŠ½å–æ¨¡å‹: {model_name}

```json
{extraction_json}
```

è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚è¾“å‡ºè¯„ä¼°åçš„ JSON,ä¸ºæ¯ä¸ªå®ä½“å’Œå…³ç³»æ·»åŠ  `evaluation` å­—æ®µã€‚
"""
    
    # è°ƒç”¨ Gemini API
    try:
        response = client.chat.completions.create(
            model=EVAL_MODEL,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ PHM é¢†åŸŸçš„çŸ¥è¯†æŠ½å–è¯„ä¼°ä¸“å®¶ã€‚åªè¾“å‡ºä¸¥æ ¼çš„ JSONï¼Œä¸æ·»åŠ ä»»ä½•è§£é‡Šã€‚"},
                {"role": "user", "content": eval_prompt}
            ],
            temperature=0,
            response_format={"type": "json_object"}
        )
        
        content = response.choices[0].message.content
        evaluated_data = parse_json_response(content)
        
        # éªŒè¯è¿”å›æ ¼å¼
        if "entities" not in evaluated_data or "relations" not in evaluated_data:
            raise ValueError(f"è¿”å›çš„ JSON ç¼ºå°‘å¿…éœ€å­—æ®µ: {evaluated_data.keys()}")
        
        return {
            "evaluated_data": evaluated_data,
            "raw_response": content,
            "usage": {
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            } if hasattr(response, 'usage') else None
        }
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        raise

# ------------------------------
# æ‰¹é‡è¯„ä¼°
# ------------------------------
def evaluate_model_results(client: OpenAI, eval_prompt_template: str, model_name: str, extraction_dir: Path, eval_output_root: Path, eval_log_dir: Path, overwrite: bool = False):
    """è¯„ä¼°å•ä¸ªæ¨¡å‹çš„æ‰€æœ‰æŠ½å–ç»“æœ"""
    
    print(f"\n{'='*80}")
    print(f"ğŸ” è¯„ä¼° {model_name} æ¨¡å‹çš„æŠ½å–ç»“æœ")
    print(f"{'='*80}")
    
    # è·å–æ‰€æœ‰ JSON æ–‡ä»¶ï¼ˆé€’å½’ï¼‰
    json_files = sorted(extraction_dir.rglob("*.json"))
    
    if not json_files:
        print(f"âš ï¸ æœªæ‰¾åˆ°ä»»ä½• JSON æ–‡ä»¶: {extraction_dir}")
        return
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(json_files)} ä¸ªæŠ½å–ç»“æœæ–‡ä»¶")
    
    # åˆ›å»ºæ¨¡å‹ä¸“ç”¨è¾“å‡ºç›®å½•ï¼ˆå¦‚ deepseek/gemini/kimiï¼‰
    model_eval_dir = eval_output_root / model_name.lower()
    os.makedirs(model_eval_dir, exist_ok=True)
    
    # ç»Ÿè®¡ä¿¡æ¯
    success_count = 0
    failed_count = 0
    total_correct_entities = 0
    total_incorrect_entities = 0
    total_correct_relations = 0
    total_incorrect_relations = 0
    
    # è¯„ä¼°æ—¥å¿—
    eval_log = []
    
    # é€ä¸ªè¯„ä¼°
    for json_file in tqdm(json_files, desc=f"è¯„ä¼° {model_name}", unit="ç¯‡"):
        paper_name = json_file.stem
        
        try:
            # è‹¥å·²æœ‰è¯„ä¼°ç»“æœä¸”æœªæŒ‡å®šè¦†ç›–ï¼Œåˆ™è·³è¿‡å¹¶çº³å…¥ç»Ÿè®¡ï¼ˆæ–­ç‚¹ç»­è·‘ï¼‰
            eval_output_file = model_eval_dir / f"{paper_name}_evaluated.json"
            if eval_output_file.exists() and not overwrite:
                try:
                    with open(eval_output_file, 'r', encoding='utf-8') as ef:
                        evaluated = json.load(ef)
                    entities_correct = sum(1 for e in evaluated.get('entities', []) if e.get('evaluation') == 'æ­£ç¡®')
                    entities_incorrect = sum(1 for e in evaluated.get('entities', []) if e.get('evaluation') == 'é”™è¯¯')
                    relations_correct = sum(1 for r in evaluated.get('relations', []) if r.get('evaluation') == 'æ­£ç¡®')
                    relations_incorrect = sum(1 for r in evaluated.get('relations', []) if r.get('evaluation') == 'é”™è¯¯')

                    total_correct_entities += entities_correct
                    total_incorrect_entities += entities_incorrect
                    total_correct_relations += relations_correct
                    total_incorrect_relations += relations_incorrect

                    log_entry = {
                        "time": now_iso(),
                        "paper": paper_name,
                        "model": model_name,
                        "status": "skipped",
                        "reason": "exists",
                        "eval_time": 0,
                        "entities": {
                            "total": len(evaluated.get('entities', [])),
                            "correct": entities_correct,
                            "incorrect": entities_incorrect,
                            "uncertain": len(evaluated.get('entities', [])) - entities_correct - entities_incorrect
                        },
                        "relations": {
                            "total": len(evaluated.get('relations', [])),
                            "correct": relations_correct,
                            "incorrect": relations_incorrect,
                            "uncertain": len(evaluated.get('relations', [])) - relations_correct - relations_incorrect
                        },
                        "usage": None,
                        "output_file": str(eval_output_file)
                    }
                    eval_log.append(log_entry)
                    tqdm.write(f"   â­ï¸ è·³è¿‡ï¼ˆå·²å­˜åœ¨ï¼‰: {paper_name}")
                    success_count += 1
                    continue
                except Exception as _e_skip:
                    tqdm.write(f"   âš ï¸ è·³è¿‡å¤±è´¥ï¼Œå°è¯•é‡è¯„: {paper_name}ï¼ˆåŸå› : {_e_skip}ï¼‰")

            # è¯»å–æŠ½å–ç»“æœ
            with open(json_file, 'r', encoding='utf-8') as f:
                extraction_data = json.load(f)
            
            # è°ƒç”¨è¯„ä¼°
            tqdm.write(f"   ğŸ“ è¯„ä¼°: {paper_name}")
            start_time = time.time()
            
            eval_result = evaluate_extraction(client, eval_prompt_template, extraction_data, paper_name, model_name)
            
            eval_time = time.time() - start_time
            
            # åªä¿å­˜è¯„ä¼°åçš„ JSON ç»“æœ
            with open(eval_output_file, 'w', encoding='utf-8') as f:
                json.dump(eval_result['evaluated_data'], f, ensure_ascii=False, indent=2)
            
            # ç»Ÿè®¡è¯„ä¼°ç»“æœ
            evaluated = eval_result['evaluated_data']
            entities_correct = sum(1 for e in evaluated.get('entities', []) if e.get('evaluation') == 'æ­£ç¡®')
            entities_incorrect = sum(1 for e in evaluated.get('entities', []) if e.get('evaluation') == 'é”™è¯¯')
            relations_correct = sum(1 for r in evaluated.get('relations', []) if r.get('evaluation') == 'æ­£ç¡®')
            relations_incorrect = sum(1 for r in evaluated.get('relations', []) if r.get('evaluation') == 'é”™è¯¯')
            
            total_correct_entities += entities_correct
            total_incorrect_entities += entities_incorrect
            total_correct_relations += relations_correct
            total_incorrect_relations += relations_incorrect
            
            # è®°å½•æ—¥å¿—
            log_entry = {
                "time": now_iso(),
                "paper": paper_name,
                "model": model_name,
                "status": "success",
                "eval_time": round(eval_time, 2),
                "entities": {
                    "total": len(evaluated.get('entities', [])),
                    "correct": entities_correct,
                    "incorrect": entities_incorrect,
                    "uncertain": len(evaluated.get('entities', [])) - entities_correct - entities_incorrect
                },
                "relations": {
                    "total": len(evaluated.get('relations', [])),
                    "correct": relations_correct,
                    "incorrect": relations_incorrect,
                    "uncertain": len(evaluated.get('relations', [])) - relations_correct - relations_incorrect
                },
                "usage": eval_result.get('usage'),
                "output_file": str(eval_output_file)
            }
            eval_log.append(log_entry)
            
            tqdm.write(f"   âœ… å®ä½“: {entities_correct}/{len(evaluated.get('entities', []))} æ­£ç¡®, "
                      f"å…³ç³»: {relations_correct}/{len(evaluated.get('relations', []))} æ­£ç¡®")
            
            success_count += 1
            
            # é¿å… API é™æµ
            time.sleep(1)
            
        except Exception as e:
            tqdm.write(f"   âŒ å¤±è´¥: {e}")
            
            # è®°å½•å¤±è´¥æ—¥å¿—
            log_entry = {
                "time": now_iso(),
                "paper": paper_name,
                "model": model_name,
                "status": "failed",
                "error": str(e)
            }
            eval_log.append(log_entry)
            
            failed_count += 1
            continue
    
    # ä¿å­˜è¯„ä¼°æ—¥å¿—
    log_file = eval_log_dir / f"{model_name.lower()}_evaluation_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(log_file, 'w', encoding='utf-8') as f:
        json.dump(eval_log, f, ensure_ascii=False, indent=2)
    
    # æ‰“å°æ±‡æ€»
    print(f"\n{'='*80}")
    print(f"ğŸ“Š {model_name} è¯„ä¼°æ±‡æ€»")
    print(f"{'='*80}")
    print(f"âœ… æˆåŠŸ: {success_count}/{len(json_files)}")
    print(f"âŒ å¤±è´¥: {failed_count}/{len(json_files)}")
    
    if success_count > 0:
        total_entities = total_correct_entities + total_incorrect_entities
        total_relations = total_correct_relations + total_incorrect_relations
        
        entity_accuracy = (total_correct_entities / total_entities * 100) if total_entities > 0 else 0
        relation_accuracy = (total_correct_relations / total_relations * 100) if total_relations > 0 else 0
        
        print(f"\nå®ä½“å‡†ç¡®ç‡: {entity_accuracy:.2f}% ({total_correct_entities}/{total_entities})")
        print(f"å…³ç³»å‡†ç¡®ç‡: {relation_accuracy:.2f}% ({total_correct_relations}/{total_relations})")
    
    print(f"\nğŸ’¾ è¯„ä¼°æ—¥å¿—: {log_file}")
    print(f"ğŸ“ è¯„ä¼°ç»“æœ: {model_eval_dir}")
    
    return {
        "model": model_name,
        "success": success_count,
        "failed": failed_count,
        "total": len(json_files),
        "entity_accuracy": entity_accuracy if success_count > 0 else 0,
        "relation_accuracy": relation_accuracy if success_count > 0 else 0,
        "correct_entities": total_correct_entities,
        "incorrect_entities": total_incorrect_entities,
        "correct_relations": total_correct_relations,
        "incorrect_relations": total_incorrect_relations
    }

def detect_model_dir(model_name: str, extractions_root: Path) -> Optional[Path]:
    name = model_name.lower()
    candidates = [
        name,
        f"{name}_rag",
    ]
    for c in candidates:
        p = extractions_root / c
        if p.exists():
            return p
    return None


# ------------------------------
# ä¸»ç¨‹åº
# ------------------------------
def main():
    parser = argparse.ArgumentParser(description="å®ä½“å…³ç³»æŠ½å–ç»“æœè´¨é‡è¯„ä¼°")
    parser.add_argument(
        "--outputs-dir",
        type=str,
        default=str(PROJECT_ROOT / "outputs"),
        help="outputs æ ¹ç›®å½•ï¼ˆåŒ…å« extractions/evaluations/logs ç­‰å­ç›®å½•ï¼‰"
    )
    parser.add_argument(
        "--models",
        type=str,
        default="DeepSeek,Gemini,Kimi",
        help="è¦è¯„ä¼°çš„æ¨¡å‹åˆ—è¡¨ï¼Œé€—å·åˆ†éš”ï¼Œä¾‹å¦‚: DeepSeek,Gemini"
    )
    parser.add_argument(
        "--overwrite",
        action="store_true",
        help="å¦‚å·²å­˜åœ¨è¯„ä¼°ç»“æœï¼Œæ˜¯å¦å¼ºåˆ¶è¦†ç›–é‡è¯„ï¼ˆé»˜è®¤è·³è¿‡ä»¥æ”¯æŒæ–­ç‚¹ç»­è·‘ï¼‰"
    )
    args = parser.parse_args()

    outputs_root = Path(args.outputs_dir).resolve()
    extractions_root = outputs_root / "extractions"
    eval_output_root = outputs_root / "evaluations"
    eval_log_dir = outputs_root / "logs" / "evaluation"
    os.makedirs(eval_output_root, exist_ok=True)
    os.makedirs(eval_log_dir, exist_ok=True)

    prompt_path = resolve_prompt_file()
    print("=" * 80)
    print("ğŸ”¬ å®ä½“å…³ç³»æŠ½å–ç»“æœè´¨é‡è¯„ä¼°")
    print("=" * 80)
    print(f"è¯„ä¼°æ¨¡å‹: {EVAL_MODEL}")
    print(f"è¯„ä¼° Prompt: {prompt_path}")
    print(f"outputs æ ¹ç›®å½•: {outputs_root}")

    print(f"ğŸ“„ åŠ è½½è¯„ä¼° Prompt: {prompt_path}")
    with open(prompt_path, 'r', encoding='utf-8') as f:
        eval_prompt_template = f.read()
    print(f"âœ… Prompt é•¿åº¦: {len(eval_prompt_template)} å­—ç¬¦\n")

    client = init_client()

    # è¯„ä¼°æŒ‡å®šæ¨¡å‹
    models = [m.strip() for m in args.models.split(',') if m.strip()]
    all_results = []

    for model_name in models:
        model_dir = detect_model_dir(model_name, extractions_root)
        if model_dir is None:
            print(f"\nâš ï¸ è·³è¿‡ {model_name}: æœªåœ¨ {extractions_root} ä¸‹æ‰¾åˆ°ç›®å½•ï¼ˆå°è¯•è¿‡ {model_name.lower()} ä¸ *_ragï¼‰")
            continue

        result = evaluate_model_results(
            client=client,
            eval_prompt_template=eval_prompt_template,
            model_name=model_name,
            extraction_dir=model_dir,
            eval_output_root=eval_output_root,
            eval_log_dir=eval_log_dir,
            overwrite=args.overwrite,
        )
        all_results.append(result)

    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    if all_results:
        print("\n" + "=" * 80)
        print("ğŸ“‹ æ¨¡å‹è¯„ä¼°å¯¹æ¯”æ±‡æ€»")
        print("=" * 80)

        import pandas as pd

        summary_df = pd.DataFrame([
            {
                "æ¨¡å‹": r["model"],
                "è¯„ä¼°æˆåŠŸæ•°": r["success"],
                "è¯„ä¼°å¤±è´¥æ•°": r["failed"],
                "å®ä½“å‡†ç¡®ç‡(%)": round(r["entity_accuracy"], 2),
                "å…³ç³»å‡†ç¡®ç‡(%)": round(r["relation_accuracy"], 2),
                "æ­£ç¡®å®ä½“æ•°": r["correct_entities"],
                "é”™è¯¯å®ä½“æ•°": r["incorrect_entities"],
                "æ­£ç¡®å…³ç³»æ•°": r["correct_relations"],
                "é”™è¯¯å…³ç³»æ•°": r["incorrect_relations"]
            }
            for r in all_results
        ])

        print(summary_df.to_string(index=False))

        # ä¿å­˜æ±‡æ€»
        summary_file = eval_output_root / f"evaluation_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        summary_df.to_csv(summary_file, index=False, encoding='utf-8-sig')
        print(f"\nâœ… è¯„ä¼°æ±‡æ€»å·²ä¿å­˜: {summary_file}")

    print("\n" + "=" * 80)
    print("âœ… è¯„ä¼°å®Œæˆ!")
    print("=" * 80)

if __name__ == "__main__":
    main()
