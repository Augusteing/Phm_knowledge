# -*- coding: utf-8 -*-
"""
ä¸€è‡´æ€§åˆ†æè„šæœ¬ï¼ˆé€‚é… EXP è¾“å‡ºç»“æ„ï¼Œæ”¯æŒå®éªŒä¸€/äºŒ/ä¸‰å¤ç”¨ï¼‰

è¾“å…¥ç›®å½•ï¼ˆç›¸å¯¹ outputs æ ¹ç›®å½•ï¼‰ï¼š
    outputs/extractions/consistency/
    â”œâ”€â”€ 1/
    â”‚   â”œâ”€â”€ deepseek/ æˆ– deepseek_rag/
    â”‚   â”œâ”€â”€ gemini/   æˆ– gemini_rag/
    â”‚   â””â”€â”€ kimi/     æˆ– kimi_rag/
    â”œâ”€â”€ 2/
    â””â”€â”€ 3/

è¾“å‡ºç›®å½•ï¼ˆä¸å…¶å®ƒåˆ†æè„šæœ¬ä¸€è‡´ï¼‰ï¼š
    outputs/analysis/consistency/
    â”œâ”€â”€ consistency_summary.md
    â”œâ”€â”€ deepseek/paper_consistency.csv
    â”œâ”€â”€ gemini/paper_consistency.csv
    â””â”€â”€ kimi/paper_consistency.csv

æŒ‡æ ‡å®šä¹‰ï¼ˆåŸºç¡€ç‰ˆï¼‰ï¼š
    - å®ä½“é›†åˆä¸€è‡´æ€§ï¼ˆJaccard å¹³å‡ï¼‰ï¼šåŒä¸€è®ºæ–‡è·¨å¤šæ¬¡è¿è¡Œçš„å®ä½“é›†åˆæˆå¯¹ Jaccard ç›¸ä¼¼åº¦çš„å¹³å‡å€¼
    - å…³ç³»é›†åˆä¸€è‡´æ€§ï¼ˆJaccard å¹³å‡ï¼‰ï¼šåŒä¸Š
    - è§„æ¨¡ç¨³å®šæ€§ï¼ˆCVï¼‰ï¼šå®ä½“æ•°ã€å…³ç³»æ•°åœ¨å¤šæ¬¡è¿è¡Œé—´çš„å˜å¼‚ç³»æ•°ï¼ˆstd/meanï¼‰

å¥å£®æ€§ï¼š
    - JSON é”®ååšå…¼å®¹æå–ï¼›å°‘äº 2 æ¬¡è¿è¡Œçš„è®ºæ–‡ï¼ŒJaccard/CV è®°ä¸º NAã€‚
    - è‡ªåŠ¨è¯†åˆ« deepseek/deepseek_rag ç­‰å­ç›®å½•å‘½åï¼›é€’å½’æ‰«æï¼Œå¹¶æ’é™¤æ—¥å¿—ä¸ *_evaluated.jsonã€‚
"""

from __future__ import annotations

import os
import json
import math
import argparse
from pathlib import Path
from itertools import combinations
from collections import defaultdict
from typing import Dict, List, Set, Tuple, Any

import pandas as pd

# å®éªŒæ ¹ç›®å½•ï¼ˆå½“å‰è„šæœ¬æ‰€åœ¨å®éªŒç›®å½•ï¼‰
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent

def resolve_outputs_root(cli_outputs_dir: str | None) -> Path:
    if cli_outputs_dir:
        return Path(cli_outputs_dir).resolve()
    return PROJECT_ROOT / "outputs"

# å¯æ¥å—çš„æ¨¡å‹å­ç›®å½•å‘½åï¼ˆå°†æŒ‰å‰ç¼€åŒ¹é…ï¼‰
MODEL_DIR_HINTS = {
    "deepseek": ["deepseek", "deepseek_rag"],
    "gemini": ["gemini", "gemini_rag"],
    "kimi": ["kimi", "kimi_rag"],
}


def norm_text(x: Any) -> str:
    if x is None:
        return ""
    s = str(x).strip().lower()
    # å¯æ‹“å±•ï¼šå…¨è§’åŠè§’ã€ç‰¹æ®Šç©ºæ ¼
    return " ".join(s.split())


def entity_key(ent: dict) -> str:
    name = ent.get("name") or ent.get("entity") or ent.get("text")
    etype = ent.get("type") or ent.get("entity_type") or ent.get("category")
    return f"{norm_text(name)}::{norm_text(etype)}"


def relation_key(rel: dict) -> str:
    # å…¼å®¹ä¸åŒå­—æ®µå‘½å
    src_name = rel.get("source_name") or rel.get("source") or rel.get("from")
    src_type = rel.get("source_type") or rel.get("from_type") or rel.get("sourceEntityType")
    tgt_name = rel.get("target_name") or rel.get("target") or rel.get("to")
    tgt_type = rel.get("target_type") or rel.get("to_type") or rel.get("targetEntityType")
    rtype = rel.get("type") or rel.get("relation") or rel.get("relation_type")
    return f"{norm_text(src_name)}::{norm_text(src_type)}->{norm_text(rtype)}->{norm_text(tgt_name)}::{norm_text(tgt_type)}"


def load_json_safely(p: Path) -> dict | None:
    try:
        with open(p, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None


def extract_sets_from_file(p: Path) -> Tuple[Set[str], Set[str], int, int]:
    data = load_json_safely(p)
    if not isinstance(data, dict):
        return set(), set(), 0, 0
    ents = data.get("entities") or []
    rels = data.get("relations") or []
    ent_set = set()
    for e in ents:
        try:
            ent_set.add(entity_key(e))
        except Exception:
            continue
    rel_set = set()
    for r in rels:
        try:
            rel_set.add(relation_key(r))
        except Exception:
            continue
    return ent_set, rel_set, len(ents) if isinstance(ents, list) else 0, len(rels) if isinstance(rels, list) else 0


def jaccard(a: Set[str], b: Set[str]) -> float:
    if not a and not b:
        return 1.0
    if not a or not b:
        return 0.0
    inter = len(a & b)
    union = len(a | b)
    if union == 0:
        return 1.0
    return inter / union


def coeff_variation(values: List[float]) -> float | None:
    if len(values) < 2:
        return None
    mean = sum(values) / len(values)
    if mean == 0:
        return None
    var = sum((v - mean) ** 2 for v in values) / (len(values) - 1)
    std = math.sqrt(var)
    return std / mean


def _find_model_dirs(run_dir: Path, model_key: str) -> List[Path]:
    """åœ¨ run_dir ä¸‹æŸ¥æ‰¾ä¸ model_key åŒ¹é…çš„å­ç›®å½•ï¼Œæ”¯æŒ deepseek/deepseek_rag ç­‰å˜ä½“ã€‚"""
    results: List[Path] = []
    if not run_dir.exists():
        return results
    hints = MODEL_DIR_HINTS.get(model_key, [model_key])
    for child in run_dir.iterdir():
        if not child.is_dir():
            continue
        name = child.name.lower()
        if any(name.startswith(h) for h in hints):
            results.append(child)
    return results

def scan_run_files(run_dir: Path, model_key: str) -> Dict[str, Path]:
    """æ‰«æå•æ¬¡è¿è¡Œä¸­æŸæ¨¡å‹çš„æ‰€æœ‰è®ºæ–‡ç»“æœæ–‡ä»¶ï¼Œè¿”å› {paper_id: path}ã€‚
    - è‡ªåŠ¨è¯†åˆ«æ¨¡å‹å­ç›®å½•ï¼ˆdeepseek ä¸ deepseek_rag ç­‰ï¼‰
    - é€’å½’ rglobï¼Œè¿‡æ»¤æ—¥å¿—ä¸ *_evaluated.json
    """
    results: Dict[str, Path] = {}
    model_dirs = _find_model_dirs(run_dir, model_key)
    for base in model_dirs:
        for p in base.rglob("*.json"):
            name_lower = p.name.lower()
            if "log" in name_lower:
                continue
            if name_lower.endswith("_evaluated.json"):
                continue
            paper = p.stem
            # è‹¥åŒåå‡ºç°å¤šæ¬¡ï¼Œåå‡ºç°çš„è¦†ç›–å‰ä¸€ä¸ªï¼ˆä¸€èˆ¬ä¸å†²çªï¼‰
            results[paper] = p
    return results


def analyze_model(model_key: str, runs: List[Path]) -> pd.DataFrame:
    # æ”¶é›†æ¯æ¬¡è¿è¡Œçš„æ–‡ä»¶æ˜ å°„
    run_maps: List[Dict[str, Path]] = [scan_run_files(r, model_key) for r in runs]

    # æ±‡æ€»æ‰€æœ‰è®ºæ–‡ID
    all_papers: Set[str] = set()
    for m in run_maps:
        all_papers.update(m.keys())

    rows = []
    for paper in sorted(all_papers):
        ent_sets: List[Set[str]] = []
        rel_sets: List[Set[str]] = []
        ent_counts: List[int] = []
        rel_counts: List[int] = []
        used_runs = 0

        for m in run_maps:
            path = m.get(paper)
            if not path:
                continue
            ent_set, rel_set, ec, rc = extract_sets_from_file(path)
            ent_sets.append(ent_set)
            rel_sets.append(rel_set)
            ent_counts.append(ec)
            rel_counts.append(rc)
            used_runs += 1

        # è®¡ç®—å¹³å‡ pairwise Jaccard
        def avg_pairwise_jacc(sets: List[Set[str]]) -> float | None:
            if len(sets) < 2:
                return None
            vals = []
            for a, b in combinations(sets, 2):
                vals.append(jaccard(a, b))
            return sum(vals) / len(vals) if vals else None

        ent_j = avg_pairwise_jacc(ent_sets)
        rel_j = avg_pairwise_jacc(rel_sets)
        ecv = coeff_variation(ent_counts)
        rcv = coeff_variation(rel_counts)

        rows.append({
            "paper": paper,
            "runs": used_runs,
            "entity_jaccard": None if ent_j is None else round(ent_j, 4),
            "relation_jaccard": None if rel_j is None else round(rel_j, 4),
            "entities_cv": None if ecv is None else round(ecv, 4),
            "relations_cv": None if rcv is None else round(rcv, 4),
            "avg_entities": round(sum(ent_counts) / used_runs, 2) if used_runs else None,
            "avg_relations": round(sum(rel_counts) / used_runs, 2) if used_runs else None,
        })

    return pd.DataFrame(rows)


def main():
    parser = argparse.ArgumentParser(description="æŠ½å–ç»“æœä¸€è‡´æ€§åˆ†æ")
    parser.add_argument(
        "--outputs-dir",
        help="è¦†ç›– outputs æ ¹ç›®å½•ï¼ˆé»˜è®¤ä½¿ç”¨å½“å‰å®éªŒç›®å½•ä¸‹çš„ outputsï¼‰",
        default=None,
    )
    args = parser.parse_args()

    outputs_root = resolve_outputs_root(args.outputs_dir)
    consistency_base = outputs_root / "extractions" / "consistency"
    print("=" * 80)
    print("ğŸ” æŠ½å–ç»“æœä¸€è‡´æ€§åˆ†æ")
    print("=" * 80)

    # æ”¶é›†æœ‰æ•ˆçš„è¿è¡Œç›®å½•ï¼ˆæ•°å­—å‘½åï¼‰
    if not consistency_base.exists():
        print(f"âŒ æœªæ‰¾åˆ°ç›®å½•: {consistency_base}")
        return

    runs = [d for d in consistency_base.iterdir() if d.is_dir() and d.name.isdigit()]
    runs.sort(key=lambda p: int(p.name))
    print(f"å‘ç°è¿è¡Œæ¬¡æ•°: {len(runs)} -> {[p.name for p in runs]}")

    output_dir = outputs_root / "analysis" / "consistency"
    output_dir.mkdir(parents=True, exist_ok=True)

    model_summaries = []
    for model_key in ["deepseek", "gemini", "kimi"]:
        df = analyze_model(model_key, runs)
        model_dir = output_dir / model_key
        model_dir.mkdir(parents=True, exist_ok=True)
        out_csv = model_dir / "paper_consistency.csv"
        df.to_csv(out_csv, index=False, encoding="utf-8-sig")
        print(f"   âœ… {model_key} ç»“æœå·²ä¿å­˜: {out_csv}")

        # ç»Ÿè®¡æ±‡æ€»ï¼ˆå¿½ç•¥ NAï¼‰
        def safe_mean(series: pd.Series) -> float | None:
            s = series.dropna()
            return round(s.mean(), 4) if len(s) else None

        summary = {
            "æ¨¡å‹": model_key,
            "è®ºæ–‡æ•°": len(df),
            "å¹³å‡å®ä½“Jaccard": safe_mean(df["entity_jaccard"]) if not df.empty else None,
            "å¹³å‡å…³ç³»Jaccard": safe_mean(df["relation_jaccard"]) if not df.empty else None,
            "å®ä½“æ•°CV(å‡å€¼)": safe_mean(df["entities_cv"]) if not df.empty else None,
            "å…³ç³»æ•°CV(å‡å€¼)": safe_mean(df["relations_cv"]) if not df.empty else None,
        }
        model_summaries.append(summary)

    # ç”Ÿæˆæ±‡æ€» Markdown
    md_file = output_dir / "consistency_summary.md"
    with open(md_file, "w", encoding="utf-8") as f:
        f.write("# æŠ½å–ç»“æœä¸€è‡´æ€§åˆ†ææŠ¥å‘Š\n\n")
        f.write(f"è¿è¡Œç›®å½•: `{consistency_base}`\n\n")
        f.write(f"å…±å‘ç° {len(runs)} æ¬¡è¿è¡Œ: {', '.join(p.name for p in runs)}\n\n")

        if model_summaries:
            f.write("## æ¨¡å‹çº§æ±‡æ€»\n\n")
            headers = ["æ¨¡å‹", "è®ºæ–‡æ•°", "å¹³å‡å®ä½“Jaccard", "å¹³å‡å…³ç³»Jaccard", "å®ä½“æ•°CV(å‡å€¼)", "å…³ç³»æ•°CV(å‡å€¼)"]
            f.write("| " + " | ".join(headers) + " |\n")
            f.write("|" + "|".join(["---"] * len(headers)) + "|\n")
            for s in model_summaries:
                row = [
                    s["æ¨¡å‹"],
                    s["è®ºæ–‡æ•°"],
                    s["å¹³å‡å®ä½“Jaccard"],
                    s["å¹³å‡å…³ç³»Jaccard"],
                    s["å®ä½“æ•°CV(å‡å€¼)"],
                    s["å…³ç³»æ•°CV(å‡å€¼)"],
                ]
                f.write("| " + " | ".join("" if v is None else str(v) for v in row) + " |\n")
            f.write("\n")

        f.write("## æ–‡ä»¶ç»“æ„\n\n")
        f.write("```\n")
        f.write("outputs/analysis/consistency/\n")
        f.write("â”œâ”€â”€ consistency_summary.md\n")
        f.write("â”œâ”€â”€ deepseek/paper_consistency.csv\n")
        f.write("â”œâ”€â”€ gemini/paper_consistency.csv\n")
        f.write("â””â”€â”€ kimi/paper_consistency.csv\n")
        f.write("```\n\n")

        f.write("## æŒ‡æ ‡è¯´æ˜\n\n")
        f.write("- å®ä½“/å…³ç³» Jaccard: è·¨è¿è¡Œçš„é›†åˆç›¸ä¼¼åº¦ï¼ˆæˆå¯¹å¹³å‡ï¼‰ï¼Œè¶Šé«˜è¶Šç¨³å®š\n")
        f.write("- å®ä½“æ•°/å…³ç³»æ•° CV: è§„æ¨¡æ³¢åŠ¨ï¼ˆstd/meanï¼‰ï¼Œè¶Šä½è¶Šç¨³å®š\n")
        f.write("- runs åˆ—ç¤ºè¯¥è®ºæ–‡å®é™…å‚ä¸ç»Ÿè®¡çš„è¿è¡Œæ¬¡æ•°ï¼ˆç¼ºå¤±ä¼šè‡ªåŠ¨è·³è¿‡ï¼‰\n")

    print(f"\nâœ… æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {md_file}")


if __name__ == "__main__":
    main()
